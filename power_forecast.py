:
#!/usr/bin/env python
# power_forecast.py  (K-Fold, 14-day history, 2-day decoder, 1-day prediction)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. model.py ì˜ 'TotalEmbedding' ì˜ì¡´ì„± ë”ë¯¸ ì£¼ì… (ìˆ˜ì • ì—†ì´ ì‚¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import sys, types, math, argparse, random
import torch, torch.nn as nn
import pandas as pd, numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from torch.utils.data import Dataset, DataLoader
from torch.nn import L1Loss
from torch.optim import AdamW
from torch.optim.lr_scheduler import StepLR
import matplotlib.pyplot as plt
from tqdm import tqdm
from typing import Optional
from torch.utils.data import ConcatDataset, Subset
from weather import weather_api
import matplotlib.dates as mdates
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Tuple, Optional
# === ADD BEGIN (ì‹ ê·œ import) ======================================
import os


# === ADD END ======================================================

class _DummyTotalEmbedding(nn.Module):
    def __init__(self, d_model, enc_in, dec_in, dropout):
        super().__init__()
        self.proj = nn.Linear(enc_in + dec_in, d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, x): return self.drop(self.proj(x))


_mod_root = types.ModuleType("models")
_mod_tfms = types.ModuleType("models.tranformers")  # ì˜¤íƒˆì(tranformers) ê·¸ëŒ€ë¡œ
_mod_file = types.ModuleType("models.tranformers.transformer")
_mod_file.TotalEmbedding = _DummyTotalEmbedding
sys.modules.update({
    "models": _mod_root,
    "models.tranformers": _mod_tfms,
    "models.tranformers.transformer": _mod_file,
})

from model import TransformerRevIN  # ì´ì œ ì˜¤ë¥˜ ì—†ì´ import


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Dataset
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class PowerDataset(Dataset):
    """7 ì¼(history) â†’ 24 h ì˜ˆì¸¡ìš© ì‹œê³„ì—´ Dataset"""

    def __init__(self, df, scaler: StandardScaler,
                 win=168, label_len=24, pred_len=24, fit=False):
        self.win, self.label_len, self.pred_len = win, label_len, pred_len
        df = df.copy()

        df["mrdDt"] = pd.to_datetime(df["mrdDt"], format="%Y-%m-%d %H")
        h = df["mrdDt"].dt.hour.astype(float)
        df["sin_h"] = np.sin(2 * math.pi * h / 24)
        df["cos_h"] = np.cos(2 * math.pi * h / 24)

        feats = ["pwrQrt", "temperature", "precipitation",
                 "windspeed", "humidity", "sin_h", "cos_h"]
        df[feats] = df[feats].apply(pd.to_numeric, errors="coerce") \
            .interpolate(method="linear", limit_direction="both") \
            .fillna(0.0)
        vals = df[feats].values
        if fit: scaler.fit(vals)
        vals = scaler.transform(vals);
        vals[np.isinf(vals)] = 0.0

        self.vals = torch.tensor(vals, dtype=torch.float32)
        self.times = df["mrdDt"].to_numpy()  # íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
        self.len = len(df) - win - pred_len + 1

    def __len__(self): return self.len

    def __getitem__(self, idx):
        s, e, p = idx, idx + self.win, idx + self.win + self.pred_len
        enc = self.vals[s:e]
        dec = self.vals[e - self.label_len:p].clone()
        dec[self.label_len:, 0] = 0.
        tgt = self.vals[e:p, 0]
        return enc, dec, tgt

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Utils
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def causal_mask(sz, device):
    return torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)


def run_epoch(model, loader, crit, opt=None, mask=None, device="cpu", desc: Optional[str] = None):
    train = opt is not None;
    model.train() if train else model.eval()
    tot, n = 0., 0
    iterable = tqdm(loader, desc=desc, leave=False)
    with torch.set_grad_enabled(train):
        for enc, dec, tgt in loader:
            enc, dec, tgt = enc.to(device), dec.to(device), tgt.to(device)
            out = model(enc, dec, tgt_mask=mask)
            if out.size(1) != tgt.size(1): out = out[:, -tgt.size(1):]
            loss = crit(out, tgt)
            if train:
                opt.zero_grad();
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0);
                opt.step()
            tot += loss.item() * tgt.numel();
            n += tgt.numel()
            iterable.set_postfix(loss=loss.item())
    return tot / n


def eval_epoch_real(model, loader, scaler, mask, device="cpu"):
    """í‰ê°€ìš©: ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì› ìŠ¤ì¼€ì¼ë¡œ ë³µì›í•˜ì—¬ MAE ê³„ì‚°"""
    model.eval()
    total_err, total_n = 0.0, 0
    mean0, std0 = scaler.mean_[0], scaler.scale_[0]

    with torch.no_grad():
        for enc, dec, tgt in loader:
            enc, dec, tgt = enc.to(device), dec.to(device), tgt.to(device)
            out = model(enc, dec, tgt_mask=mask)
            if out.size(1) != tgt.size(1):
                out = out[:, -tgt.size(1):]

            # ì› ìŠ¤ì¼€ì¼ë¡œ ë³µì›
            out_KW = out.cpu().numpy() * std0 + mean0
            tgt_KW = tgt.cpu().numpy() * std0 + mean0

            # MAE ê³„ì‚°
            total_err += np.abs(out_KW - tgt_KW).sum()
            total_n += tgt_KW.size

    return total_err / total_n


def preview_batch(model, loader, mask, scaler, device="cpu"):
    ds = loader.dataset
    win, pred_len = ds.win, ds.pred_len
    mean, std = scaler.mean_, scaler.scale_

    model.eval()
    with torch.no_grad():
        enc, dec, tgt = next(iter(loader))
        enc, dec = enc.to(device), dec.to(device)
        out = model(enc, dec, tgt_mask=mask)
        if out.size(1) != tgt.size(1):
            out = out[:, -tgt.size(1):]

        pred = (out.cpu() * std[0] + mean[0]).numpy()[0]  # pwrQrt
        true = (tgt * std[0] + mean[0]).numpy()[0]

        # ì› ìŠ¤ì¼€ì¼ì˜ ì¶”ê°€ í”¼ì²˜ (dataset ì €ì¥ëœ ìŠ¤ì¼€ì¼ ê°’ ë³µì›)
        block = ds.vals[win: win + pred_len].numpy()  # [24, 7]
        temp = block[:, 1] * std[1] + mean[1]
        prec = block[:, 2] * std[2] + mean[2]
        wind = block[:, 3] * std[3] + mean[3]
        humi = block[:, 4] * std[4] + mean[4]

        times = ds.times[win: win + pred_len]
        lines = []
        for t, p, r, tmp, pr, wd, hm in zip(
                times, pred, true, temp, prec, wind, humi):
            lines.append(
                f"{pd.to_datetime(t).strftime('%Y-%m-%d %H:%M')}  "
                f"pred={p:.4f} | true={r:.4f} | "
                f"T={tmp:.1f}Â°C  Pcp={pr:.1f}mm  W={wd:.1f}m/s  H={hm:.0f}%")
        return "\n".join(lines)


def safe_tensor(t):
    """NaN/Infê°€ ìˆëŠ” í…ì„œë¥¼ 0ìœ¼ë¡œ ëŒ€ì²´"""
    if torch.isnan(t).any() or torch.isinf(t).any():
        t = torch.nan_to_num(t, nan=0.0, posinf=0.0, neginf=0.0)
    return t


# === ADD BEGIN : ë¬´ì‘ìœ„ ë¸”ë¡ ì˜ˆì¸¡ & ì‹œê°í™” =========================
@torch.no_grad()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ì¶”ê°€ ê¸°ëŠ¥: ëœë¤ ë¸”ë¡ ì˜ˆì¸¡ì„ ìœ„í•œ predict_blocks í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict_blocks(model, df_sorted, scaler,
                   win, label_len, pred_len,
                   n_blocks=5, device="cpu", seed=42):
    """
    model     : í•™ìŠµëœ TransformerRevIN
    df_sorted : mrdDt ê¸°ì¤€ ì •ë ¬ëœ DataFrame (sin_h, cos_h í¬í•¨)
    scaler    : StandardScaler (fitì€ train setì—ì„œë§Œ)
    win       : íˆìŠ¤í† ë¦¬ ìœˆë„ìš° ê¸¸ì´ (ì˜ˆ: 168)
    label_len : ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ì“°ëŠ” ê³¼ê±° íƒ€ê¹ƒ ê¸¸ì´ (ì˜ˆ: 24)
    pred_len  : ì˜ˆì¸¡ ê¸¸ì´ (ì˜ˆ: 24)
    n_blocks  : ëœë¤í•˜ê²Œ ë½‘ì„ ë¸”ë¡ ìˆ˜
    """
    # 1) í•„ìˆ˜ íŒŒìƒ í”¼ì²˜
    if "sin_h" not in df_sorted.columns or "cos_h" not in df_sorted.columns:
        h = pd.to_datetime(df_sorted["mrdDt"]).dt.hour.astype(float)
        df_sorted["sin_h"] = np.sin(2 * np.pi * h / 24)
        df_sorted["cos_h"] = np.cos(2 * np.pi * h / 24)

    dates = df_sorted["date"].unique()
    hist_days = win // 24
    if len(dates) < hist_days + 1:
        print("[Warn] ë°ì´í„°ê°€ ë¶€ì¡±í•´ì„œ ì˜ˆì¸¡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 2) ëœë¤ ë¸”ë¡ ì‹œì‘ì¼ ìƒ˜í”Œë§
    random.seed(seed)
    max_start = len(dates) - (hist_days + 1)
    starts = random.sample(range(0, max_start + 1),
                           k=min(n_blocks, max_start + 1))

    feats = ["pwrQrt", "temperature", "precipitation",
             "windspeed", "humidity", "sin_h", "cos_h"]
    mean0, std0 = scaler.mean_[0], scaler.scale_[0]

    print(f"\n===== predict_blocks: {n_blocks}ê°œ ëœë¤ ë¸”ë¡ ì˜ˆì¸¡ =====")
    for no, s in enumerate(starts, 1):
        tgt_date = dates[s + hist_days]
        # target ë‚ ì§œì˜ ì²« row index
        idxs = df_sorted.index[df_sorted["date"] == tgt_date]
        if len(idxs) == 0:
            print(f"[Block {no:02d}] {tgt_date}  â†’ ë‚ ì§œ ë¯¸ë°œê²¬, ìŠ¤í‚µ")
            continue
        first_idx = idxs[0]
        start_row = first_idx - win
        end_row = first_idx + pred_len

        # ë²”ìœ„ ì²´í¬
        if start_row < 0 or end_row > len(df_sorted):
            print(f"[Block {no:02d}] {tgt_date}  â†’ ë²”ìœ„ ì´ˆê³¼, ìŠ¤í‚µ")
            continue

        # ì¸ì½”ë”/ë””ì½”ë” ì…ë ¥ ì¤€ë¹„
        enc_df = df_sorted.iloc[start_row: first_idx]
        dec_df = df_sorted.iloc[first_idx - label_len: end_row]

        # ìŠ¤ì¼€ì¼ë§ & NaN/Inf ì²˜ë¦¬
        enc_np = scaler.transform(enc_df[feats])
        dec_np = scaler.transform(dec_df[feats])
        enc_np = np.nan_to_num(enc_np, nan=0.0, posinf=0.0, neginf=0.0)
        dec_np = np.nan_to_num(dec_np, nan=0.0, posinf=0.0, neginf=0.0)

        # ë¯¸ë˜ íƒ€ê¹ƒ 0 ë§ˆìŠ¤í‚¹
        dec_np[label_len:, 0] = 0.0

        # í…ì„œ ë³€í™˜
        enc = torch.tensor(enc_np, dtype=torch.float32).unsqueeze(0).to(device)
        dec = torch.tensor(dec_np, dtype=torch.float32).unsqueeze(0).to(device)

        # ì˜ˆì¸¡
        with torch.no_grad():
            out = model(enc, dec,
                        tgt_mask=causal_mask(label_len + pred_len, device))
            # ë§ˆì§€ë§‰ pred_len ì‹œì ë§Œ
            pred_seq = out[0, -pred_len:]  # shape: [pred_len]

        # ë§Œì•½ NaN ì„ì—¬ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if torch.isnan(pred_seq).any():
            print(f"[Block {no:02d}] {tgt_date}  â†’ NaN ì˜ˆì¸¡, ìŠ¤í‚µ")
            continue

        # ì—­ìŠ¤ì¼€ì¼ë§
        pred = (pred_seq.cpu().numpy() * std0) + mean0
        true = df_sorted.iloc[first_idx: first_idx + pred_len]["pwrQrt"].values

        # ì¶œë ¥ & ì‹œê°í™”
        ts = pd.date_range(f"{tgt_date} 00:00", periods=pred_len, freq="1H")
        mae = np.mean(np.abs(pred - true))
        print(f"\n[Block {no:02d}] {tgt_date}  MAE={mae:.4f}")
        for t, p, y in zip(ts, pred, true):
            print(f"{t:%Y-%m-%d %H:%M}  pred={p:.4f} | true={y:.4f}")

        plt.figure(figsize=(8, 6))
        plt.plot(ts, true, label="True", linewidth=2)
        plt.plot(ts, pred, label="Pred", linewidth=2)
        plt.title(f"{tgt_date}  MAE={mae:.3f}")
        plt.ylim(0, 2)
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.show()


# === ADD END ======================================================

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--csv", required=False, type=Path)
    p.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    p.add_argument("--epochs", type=int, default=6)
    p.add_argument("--batch", type=int, default=32)
    p.add_argument("--kfold", type=int, default=2)
    # === ADD BEGIN (ìƒˆ ì¸ì) ======================================
    p.add_argument("--save_dir", type=Path, default=Path("saved_models"))
    p.add_argument("--predict_only", action="store_true")
    p.add_argument("--member_id", default=None)
    p.add_argument("--n_pred_blocks", type=int, default=5)
    p.add_argument("--pretrain_folder", type=Path, default=None,
                   help="ì—¬ê¸°ì— ì§€ì •ëœ í´ë” ì•ˆì˜ ëª¨ë“  CSVë¡œ pre-training")
    p.add_argument("--pretrain_epochs", type=int, default=10,
                   help="pre-training ì‹œí‚¬ epoch ìˆ˜")
    p.add_argument("--model", type=Path, default=None)
    p.add_argument("--finetune", type=Path, default=None)
    p.add_argument("--tomorrow", type=bool, default=False)
    p.add_argument("--yesterday", action="store_true")
    # === ADD END ==================================================
    return p.parse_args()


# (eval_random_10_blocks í•¨ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ â€• ìƒëµ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìµœê·¼ 10ê°œ(=80ì¼) ë¸”ë¡: 7ì¼ history â†’ 1ì¼(24h) ì˜ˆì¸¡ & MAE ê³„ì‚°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def eval_random_10_blocks(model, df_sorted, scaler,
                          win=168, label_len=24, pred_len=24,
                          device="cpu", seed=42):
    # 0) í•„ìˆ˜ íŒŒìƒ í”¼ì²˜
    if "sin_h" not in df_sorted.columns or "cos_h" not in df_sorted.columns:
        h = pd.to_datetime(df_sorted["mrdDt"]).dt.hour.astype(float)
        df_sorted["sin_h"] = np.sin(2 * np.pi * h / 24)
        df_sorted["cos_h"] = np.cos(2 * np.pi * h / 24)

    dates = df_sorted["date"].unique()
    if len(dates) < 8:
        print("[Warn] 8 ì¼ ì´í•˜ ë°ì´í„° â€” í‰ê°€ ìƒëµ");
        return

    # ë¬´ì‘ìœ„ 10ê°œ ì‹œì‘ì 
    random.seed(seed)
    starts = random.sample(range(0, len(dates) - 8 + 1),
                           k=min(10, len(dates) - 7))

    feats = ["pwrQrt", "temperature", "precipitation",
             "windspeed", "humidity", "sin_h", "cos_h"]
    mean0, std0 = scaler.mean_[0], scaler.scale_[0]

    print("\n===== ë¬´ì‘ìœ„ 10ë¸”ë¡(7+1ì¼) ì˜ˆì¸¡ =====")
    agg_abs_err, agg_n = 0.0, 0
    plot_data = []  # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš©
    for no, s in enumerate(starts, 1):
        tgt_date = dates[s + 7]
        start_row = df_sorted.index[df_sorted["date"] == tgt_date][0] - win
        if start_row < 0 or start_row + win + pred_len > len(df_sorted):
            print(f"[Block {no:02d}] {tgt_date}  â†’ ë²”ìœ„ ì´ˆê³¼, ìŠ¤í‚µ")
            continue

        enc_df = df_sorted.iloc[start_row: start_row + win]
        dec_df = df_sorted.iloc[start_row + win - label_len: start_row + win + pred_len].copy()

        # ìŠ¤ì¼€ì¼ë§ + NaN/Inf ì •ë¦¬
        enc_np = np.nan_to_num(scaler.transform(enc_df[feats]),
                               nan=0.0, posinf=0.0, neginf=0.0)
        dec_np = np.nan_to_num(scaler.transform(dec_df[feats]),
                               nan=0.0, posinf=0.0, neginf=0.0)
        dec_np[label_len:, 0] = 0.0

        enc = torch.tensor(enc_np, dtype=torch.float32).unsqueeze(0).to(device)
        dec = torch.tensor(dec_np, dtype=torch.float32).unsqueeze(0).to(device)
        tgt_raw = df_sorted.iloc[start_row + win: start_row + win + pred_len]["pwrQrt"].values
        tgt = torch.tensor((tgt_raw - mean0) / std0, dtype=torch.float32)

        with torch.no_grad():
            out = model(enc, dec,
                        tgt_mask=causal_mask(label_len + pred_len, device))
            out = out.squeeze(0)[-pred_len:]
        if torch.isnan(out).any():
            print(f"[Block {no:02d}] {tgt_date}  â†’ NaN ë°œìƒ, ìŠ¤í‚µ")
            continue

        pred = (out.cpu() * std0 + mean0).numpy()
        true = (tgt * std0 + mean0).numpy()
        blk_mae = np.mean(np.abs(pred - true))
        agg_abs_err += np.abs(pred - true).sum();
        agg_n += pred_len

        feat_rows = df_sorted.iloc[start_row + win: start_row + win + pred_len]
        temp = feat_rows["temperature"].values
        prec = feat_rows["precipitation"].values
        wind = feat_rows["windspeed"].values
        humi = feat_rows["humidity"].values
        print(f"\n[Block {no:02d}] {tgt_date}  MAE={blk_mae:.4f}")
        times = pd.date_range(f"{tgt_date} 00:00", periods=24, freq="1H")
        plot_data.append({
            "block": no, "times": times, "pred": pred, "true": true})

        for ts, p, v, tmp, pr, wd, hm in zip(
                times, pred, true, temp, prec, wind, humi):
            print(f"{ts.strftime('%Y-%m-%d %H:%M')}  "
                  f"pred={p:.4f} | true={v:.4f} | "
                  f"T={tmp:.1f}Â°C   Pcp={pr:.1f}mm  W={wd:.1f}m/s  H={hm:.0f}%")

    for item in plot_data:
        plt.figure(figsize=(8, 6))
        plt.plot(item["times"], item["true"], label="True", linewidth=2)
        plt.plot(item["times"], item["pred"], label="Pred", linewidth=2)
        plt.ylim(0, 3)
        plt.title(f"Block {item['block']:02d} â€“ {item['times'][0].date()}  (7ì¼â†’1ì¼)")
        plt.xlabel("Time");
        plt.ylabel("pwrQrt (kW)")
        plt.xticks(rotation=45);
        plt.legend();
        plt.tight_layout()
        plt.show()
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if agg_n:
        print(f"\n--> 10ë¸”ë¡ í‰ê·  MAE = {agg_abs_err / agg_n:.4f}\n")
    if agg_n:
        print(f"\n--> 10ë¸”ë¡ í‰ê·  MAE = {agg_abs_err / agg_n:.4f}\n")
    else:
        print("\nâ†’ 10ê°œ ë¸”ë¡ ëª¨ë‘ NaN ìœ¼ë¡œ ìŠ¤í‚µë˜ì–´ í‰ê·  MAE ë¶ˆê°€\n")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Pre-training on folder
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def pretrain_on_folder(folder: Path, args):
    """
    í´ë” ì•ˆì˜ ëª¨ë“  CSVë¥¼ ì½ì–´ì„œ í•©ì¹œ ë’¤ì— í•œ ë²ˆì— í•™ìŠµ(pre-training)í•˜ê³ 
    'pretraining.pt' ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    from pathlib import Path
    feats = ["pwrQrt", "temperature", "precipitation",
             "windspeed", "humidity", "sin_h", "cos_h"]
    # 1) í´ë” ë‚´ CSV íŒŒì¼ ëª©ë¡
    csvs = sorted(Path(folder).glob("*.csv"))
    if not csvs:
        print(f"[Warn] '{folder}' ì•ˆì— CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    dfs = []
    for f in csvs:  # csv í•˜ë‚˜ = ê°€êµ¬ í•˜ë‚˜ë¼ëŠ” ì „ì œ
        df0 = pd.read_csv(f)
        df0["mrdDt"] = pd.to_datetime(df0["mrdDt"], format="%Y-%m-%d %H")

        # â”€â”€ â‘  30 % ì—°ì† ìƒ˜í”Œë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        keep_n = int(len(df0) * 0.30)
        start = random.randint(0, len(df0) - keep_n)
        df0 = df0.iloc[start:start + keep_n]  # ì—°ì† êµ¬ê°„ ì„ íƒ

        # â”€â”€ â‘¡ ê°€êµ¬ ë‚´ë¶€ ì‹œê°„ìˆœ ì •ë ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        df0 = df0.sort_values("mrdDt").reset_index(drop=True)

        # â”€â”€ â‘¢ íŒŒìƒ í”¼ì²˜ ì¶”ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if "sin_h" not in df0.columns or "cos_h" not in df0.columns:
            h = df0["mrdDt"].dt.hour.astype(float)
            df0["sin_h"] = np.sin(2 * np.pi * h / 24)
            df0["cos_h"] = np.cos(2 * np.pi * h / 24)

        dfs.append(df0)

    # â”€â”€ â‘£ ê°€êµ¬ë“¤ ì´ì–´ë¶™ì´ê¸°(ê°€êµ¬ ìˆœì„œ ê·¸ëŒ€ë¡œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_all = pd.concat(dfs, ignore_index=True)

    # â”€â”€ â‘¤ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    scaler = StandardScaler().fit(
        df_all[feats].apply(pd.to_numeric, errors="coerce")
    )
    print(f"pre-treining: {len(csvs)} ê°€êµ¬")
    # 2) â”€â”€ ê°€êµ¬(íŒŒì¼)ë³„ë¡œ Dataset ë§Œë“¤ê¸°  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ds_list = []
    for f in csvs:
        df0 = pd.read_csv(f)
        df0["mrdDt"] = pd.to_datetime(df0["mrdDt"], format="%Y-%m-%d %H")
        df0 = df0.sort_values("mrdDt").reset_index(drop=True)
        ds_list.append(
            PowerDataset(df0, scaler,
                         win=args.win, label_len=args.label,
                         pred_len=args.pred, fit=False)
        )

    full_ds = ConcatDataset(ds_list)
    dl = DataLoader(full_ds,
                    batch_size=args.batch,
                    shuffle=True,
                    num_workers=os.cpu_count(),
                    pin_memory=True)

    # 4) ëª¨ë¸ ì´ˆê¸°í™”
    model = TransformerRevIN(d_model=160,
                             input_features_count=7,
                             num_encoder_layers=3,
                             num_decoder_layers=3,
                             dim_feedforward=160,
                             kernel_size=3,
                             dropout=0.1,
                             attention_heads=8).to(args.device)
    model.relu = nn.Identity()
    print(f"Model: {model.__class__.__name__}")
    crit = L1Loss()
    opt = AdamW(model.parameters(), lr=5e-4, eps=1e-9)
    mask = causal_mask(args.label + args.pred, args.device)

    # 5) Pre-training ë£¨í”„
    print(f"\n=== Pre-training on folder '{folder}' for {args.pretrain_epochs} epochs ===")
    for ep in range(1, args.pretrain_epochs + 1):
        loss = run_epoch(model, dl, crit, opt, mask, device=args.device, desc=f"PT{ep}/ {args.pretrain_epochs}")
        print(f"[PT {ep:03d}] train(norm)={loss:.4f}")

    # 6) ê°€ì¤‘ì¹˜ & ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    args.save_dir.mkdir(parents=True, exist_ok=True)
    ckpt = args.save_dir / "pretraining.pt"
    torch.save({
        "weights": model.state_dict(),
        "scaler_mean": scaler.mean_,
        "scaler_scale": scaler.scale_
    }, ckpt)
    print(f"\nâœ… Pre-training complete â€” saved to {ckpt}\n")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Predict from CSV

def predict_from_csv(
        csv_path: Path,
        ckpt_path: Path,
        n_blocks: int = 5,
        device: str = "cpu"):
    """
    ì €ì¥ëœ .pt ì™€ CSV í•˜ë‚˜ë¥¼ ë°›ì•„ì„œ predict_blocks ë§Œ ìˆ˜í–‰í•œë‹¤.
    """
    if not ckpt_path.exists():
        sys.exit(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ â†’ {ckpt_path}")
    if not csv_path.exists():
        sys.exit(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ â†’ {csv_path}")

    # 1) ëª¨ë¸Â·ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ --------------------------------------------------
    state = torch.load(ckpt_path, map_location=device)
    model = TransformerRevIN(
        d_model=160, input_features_count=7,
        num_encoder_layers=3, num_decoder_layers=3,
        dim_feedforward=160, kernel_size=3,
        dropout=0.1, attention_heads=8
    ).to(device)
    model.load_state_dict(state["weights"])
    model.eval()

    scaler = StandardScaler()
    scaler.mean_, scaler.scale_ = state["scaler_mean"], state["scaler_scale"]

    # 2) CSV ë¡œë“œ & ì „ì²˜ë¦¬ ----------------------------------------------------
    df = pd.read_csv(csv_path)
    df["mrdDt"] = pd.to_datetime(df["mrdDt"], format="%Y-%m-%d %H")
    df = df.sort_values("mrdDt").reset_index(drop=True)
    df["date"] = df["mrdDt"].dt.date  # predict_blocks í˜¸í™˜ í•„ë“œ

    # 3) ì˜ˆì¸¡ ---------------------------------------------------------------
    hist_days = 28
    win = hist_days * 24
    label_len = 7 * 24
    pred_len = 24
    predict_blocks(model, df, scaler,
                   win, label_len, pred_len,
                   n_blocks=n_blocks,
                   device=device)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚´ì¼ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict_tomorrow(member_id: str,
                     hist_csv: Path,
                     model_path: Path,
                     device: str = "cpu"):
    tomorrow_weather = weather_api()
    print(type(tomorrow_weather))
    st = torch.load(model_path, map_location=device)
    mdl = TransformerRevIN(d_model=160, input_features_count=7,
                           num_encoder_layers=3, num_decoder_layers=3,
                           dim_feedforward=160, kernel_size=3,
                           dropout=0.05, attention_heads=8).to(device)
    mdl.load_state_dict(st["weights"]);
    mdl.eval()

    scl = StandardScaler()
    scl.mean_, scl.scale_ = st["scaler_mean"], st["scaler_scale"]

    # â‘¢ ê³¼ê±° CSV
    df_all = pd.read_csv(hist_csv)

    # â‘£ ì˜ˆì¸¡ (ê·¸ë˜í”„ ìë™ ì¶œë ¥)
    return forecast_next_day_by_house(mdl, df_all, member_id,
                                      tomorrow_weather, scl, device=device)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# forecast_next_day_by_house
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict_tomorrow(member_id: str,
                     hist_csv: Path,
                     model_path: Path,
                     device: str = "cpu"):
    tomorrow_weather = weather_api()
    print(type(tomorrow_weather))
    st = torch.load(model_path, map_location=device)
    mdl = TransformerRevIN(d_model=160, input_features_count=7,
                           num_encoder_layers=3, num_decoder_layers=3,
                           dim_feedforward=160, kernel_size=3,
                           dropout=0.05, attention_heads=8).to(device)
    mdl.load_state_dict(st["weights"]);
    mdl.eval()

    scl = StandardScaler()
    scl.mean_, scl.scale_ = st["scaler_mean"], st["scaler_scale"]

    # â‘¢ ê³¼ê±° CSV
    df_all = pd.read_csv(hist_csv)

    # â‘£ ì˜ˆì¸¡ (ê·¸ë˜í”„ ìë™ ì¶œë ¥)
    return forecast_next_day_by_house(mdl, df_all, member_id,
                                      tomorrow_weather, scl, device=device, hist_csv_path=hist_csv)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# forecast_next_day_by_house
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def forecast_next_day_by_house(
        model: TransformerRevIN,
        df_all: pd.DataFrame,
        member_id: str,
        tomorrow_weather: pd.DataFrame,
        scaler: StandardScaler,
        device: str = "cpu",
        plot: bool = True,
        hist_csv_path: Optional[Path] = None
):
    # --- 0) memberID ì»¬ëŸ¼ íƒìƒ‰ (ëŒ€Â·ì†Œë¬¸ì ë¬´ì‹œ) --------------------
    col = next((c for c in df_all.columns if c.lower() == "memberid"), None)
    if col is None:
        print("âŒ  CSV ì— memberID ì—´ì´ ì—†ìŠµë‹ˆë‹¤.");
        return None

    # --- 1) ë¬¸ìì—´ ì „ì²˜ë¦¬ : strip + zero-padding -------------------
    df_all[col] = df_all[col].astype(str).str.strip()
    mid = str(member_id).strip()

    # (ê°€ì¥ í”í•œ) ì•ìë¦¬ 0 ëˆ„ë½ ëŒ€ë¹„ â€“ ê¸¸ì´ë¥¼ ë§ì¶° íŒ¨ë”©
    max_len = max(df_all[col].str.len().max(), len(mid))
    df_all[col] = df_all[col].str.zfill(max_len)
    mid = mid.zfill(max_len)

    # --- 2) ëŒ€ìƒ ê°€êµ¬ ì¶”ì¶œ ----------------------------------------
    df = df_all[df_all[col] == mid].copy()
    if df.empty:
        print(f"âŒ  '{member_id}' ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.");
        return None

    # --- 3) ì‹œê°„ ìˆœ ì •ë ¬ & ì¸ì½”ë”/ë””ì½”ë” êµ¬ì„± ----------------------
    df["mrdDt"] = pd.to_datetime(df["mrdDt"])
    df = df.sort_values("mrdDt").reset_index(drop=True)

    last_ts = df["mrdDt"].iloc[-1]  # â† ì´ ì‹œì ë¶€í„°ëŠ” ì•ˆì „
    next_date = (last_ts + pd.Timedelta(days=1)).date()

    # â”€â”€â”€ 1) Encoder / Decoder ì…ë ¥ ë§Œë“¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    enc_df = df.iloc[-win:]  # 28ì¼
    dec_hist = enc_df.iloc[-label:]  # 7ì¼

    # enc_df ì‹œê°„ íŒŒìƒ í”¼ì²˜ ë³´ê°•
    h = enc_df["mrdDt"].dt.hour.astype(float)
    enc_df.loc[:, "sin_h"] = np.sin(2 * np.pi * h / 24)
    enc_df.loc[:, "cos_h"] = np.cos(2 * np.pi * h / 24)

    # â”€â”€â”€ ë‚´ì¼ ë‚ ì”¨ â†’ feature ì„¸íŠ¸ ë§ì¶”ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tw = tomorrow_weather.copy()
    tw.rename(columns={"Forecast_time": "mrdDt"}, inplace=True)
    tw["mrdDt"] = pd.to_datetime(tw["mrdDt"])
    tw.loc[:, "sin_h"] = np.sin(2 * np.pi * tw["mrdDt"].dt.hour / 24)
    tw.loc[:, "cos_h"] = np.cos(2 * np.pi * tw["mrdDt"].dt.hour / 24)
    tw["pwrQrt"] = 0.0  # ë¯¸ë˜ target ë§ˆìŠ¤í‚¹

    feats = ["pwrQrt", "temperature", "precipitation",
             "windspeed", "humidity", "sin_h", "cos_h"]

    # â˜… NaN/Inf â†’ 0.0 ì¹˜í™˜ â˜…
    enc_np = np.nan_to_num(
        scaler.transform(enc_df[feats]),
        nan=0.0, posinf=0.0, neginf=0.0)

    dec_np = np.nan_to_num(
        scaler.transform(pd.concat([dec_hist, tw])[feats]),
        nan=0.0, posinf=0.0, neginf=0.0)

    enc = torch.tensor(enc_np, dtype=torch.float32).unsqueeze(0).to(device)
    dec = torch.tensor(dec_np, dtype=torch.float32).unsqueeze(0).to(device)

    mask = causal_mask(label + 24, device)
    with torch.no_grad():
        out = model(enc, dec, tgt_mask=mask)[0, -24:]
        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)  # ì•ˆì „ë§

    pred_kw = out.cpu().numpy() * scaler.scale_[0] + scaler.mean_[0]
    idx = pd.date_range(next_date, periods=24, freq="1H")
    series = pd.Series(pred_kw, index=idx, name="pred_pwrQrt")

    original_cols = df_all.columns.tolist()
    ts_tsr = series.index.strftime("%Y-%m-%d %H")
    out_df = pd.DataFrame({
        "mrdDt": ts_tsr,
        "memberID": mid,
        "pwrQrt": series.values,
        "temperature": tw["temperature"].values,
        "precipitation": tw["precipitation"].values,
        "windspeed": tw["windspeed"].values,
        "humidity": tw["humidity"].values
    })
    out_df = out_df[original_cols]
    if hist_csv_path is not None:
        out_df.to_csv(hist_csv_path, mode="a", header=False, index=False, encoding="utf-8-sig")
        print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ '{hist_csv_path}' ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("â—ï¸ CSV ì €ì¥ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì¶œë ¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # â”€â”€â”€ 2) ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if plot:
        fig, ax1 = plt.subplots(figsize=(10, 4))
        ax1.plot(series.index, series.values, marker='o',
                 label="Predicted pwrQrt (kW)")

        # â–¶â–¶ ì—¬ê¸°ë¶€í„° ìƒˆ ì½”ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 24 h ëˆˆê¸ˆ(1 h ê°„ê²©) & HH:MM í¬ë§·
        ax1.xaxis.set_major_locator(mdates.HourLocator(interval=1))
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
        ax1.set_xlim(series.index[0], series.index[-1])  # ë²”ìœ„ë¥¼ ë”± í•˜ë£¨ë¡œ ê³ ì •
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        ax1.set_ylabel("kW");
        ax1.set_xlabel("Time")
        ax1.set_title(f"{member_id} â€“ {series.index[0].date()} power forecast")
        ax1.grid(True)

        # (ì˜¨ë„ ë³´ì¡°ì¶•ì€ ê·¸ëŒ€ë¡œ)
        if "temperature" in tomorrow_weather.columns:
            ax2 = ax1.twinx()
            ax2.plot(tw["mrdDt"], tw["temperature"],
                     color="tab:red", alpha=.4, label="Temperature (Â°C)")
            ax2.set_ylabel("Â°C", color="tab:red")
            ax2.tick_params(axis='y', labelcolor="tab:red")

        fig.tight_layout()
        plt.show()

    return series

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ì˜ˆì¸¡ â†” ì‹¤ì¸¡ â‘  ì˜¤ì°¨ ê³„ì‚° â‘¡ ì˜ˆì¸¡ í–‰ ì œê±° â‘¢ CSV ê°±ì‹ 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def reconcile_day_duplicates(csv_path: Path,
                             member_id: str) -> float:
    """
    Â· ë§ˆì§€ë§‰ ì…ë ¥ í–‰ì˜ ë‚ ì§œ(y_date)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ
      ê·¸ ë‚ (00~23ì‹œ) ì•ˆì—ì„œ
        - ì¤‘ë³µ(mrdDt)ì¸ ëª¨ë“  ì˜ˆì¸¡Â·ì‹¤ì¸¡ ìŒì„ ì°¾ê³ 
        - ë§¨ ì• = ì˜ˆì¸¡, ë§¨ ë’¤ = ì‹¤ì¸¡ìœ¼ë¡œ ê°€ì •
        - abs_errë¥¼ ê³„ì‚°Â·ì¶œë ¥
        - ì˜ˆì¸¡í–‰(ì•ë¶€ë¶„) ì „ë¶€ ì‚­ì œ, ì‹¤ì¸¡ë§Œ ë‚¨ê¹€
    Â· ë°˜í™˜ : í•´ë‹¹ ë‚ ì§œ MAE  (ì˜ˆì¸¡ì´ ì—†ëŠ” ì‹œê°ì€ ì œì™¸)
    """
    df = pd.read_csv(csv_path)
    col = next((c for c in df.columns if c.lower() == "memberid"), None)
    if col is None:
        print("âŒ memberID ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."); return 0.0

    df[col] = df[col].astype(str).str.strip()
    member_id = str(member_id).strip()
    df["mrdDt"] = pd.to_datetime(df["mrdDt"])

    # â”€â”€ ëŒ€ìƒ ê°€êµ¬ Â· ë§ˆì§€ë§‰ ì…ë ¥ ë‚ ì§œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    dfi = df[df[col] == member_id]
    if dfi.empty:
        print(f"âŒ {member_id} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return 0.0

    y_date = dfi.iloc[-1]["mrdDt"].date()        # ë§ˆì§€ë§‰ í–‰ ë‚ ì§œ
    day_mask = dfi["mrdDt"].dt.date == y_date
    dfd = dfi[day_mask].copy()

    if dfd.empty:
        print(f"âš ï¸  {y_date} ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return 0.0

    abs_err, drop_idx = [], []

    # â”€â”€ ì‹œê°ë³„ ì˜ˆì¸¡Â·ì‹¤ì¸¡ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for ts, grp in dfd.groupby("mrdDt"):
        if len(grp) < 2:              # ì˜ˆì¸¡ë§Œ ìˆê±°ë‚˜ ì‹¤ì¸¡ë§Œ 1ê°œ â†’ ìŠ¤í‚µ
            continue

        pred_row = grp.iloc[0]
        real_row = grp.iloc[-1]

        err = abs(pred_row["pwrQrt"] - real_row["pwrQrt"])
        abs_err.append(err)

        print(f"[{ts:%Y-%m-%d %H:%M}] "
              f"pred={pred_row['pwrQrt']:.4f} | "
              f"real={real_row['pwrQrt']:.4f} â†’ abs_err={err:.4f}")

        # ì˜ˆì¸¡í–‰Â·ì¤‘ê°„í–‰ ì „ë¶€ ì‚­ì œ â†’ ì‹¤ì¸¡(ë§ˆì§€ë§‰) í•œ í–‰ë§Œ ë‚¨ê¹€
        drop_idx.extend(grp.index[:-1])

    # â”€â”€ CSV ê°±ì‹  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if drop_idx:
        df.drop(index=drop_idx, inplace=True)
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        print(f"ğŸ“ ì˜ˆì¸¡ ë“± {len(drop_idx)}ê°œ í–‰ ì‚­ì œ í›„ CSV ì €ì¥ ì™„ë£Œ")

    mae = np.mean(abs_err) if abs_err else 0.0
    print(f"\nğŸŒ™ {y_date} ì „ì²´ MAE = {mae:.4f} kW")
    return mae


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main (8-day block K-Fold)  â€» ë‹¹ì‹  ì½”ë“œ ê·¸ëŒ€ë¡œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
historty_days = 28
win = historty_days * 24  # 28ì¼ history
decoder_days = 7  # ë””ì½”ë” context 14ì¼
label = decoder_days * 24  # 14ì¼(336h) ë ˆì´ë¸”
pred = 24  # 1ì¼(24h) ì˜ˆì¸¡
block_days = historty_days + 1  # 14ì¼ + 1ì¼


def main():
    args = parse_args();
    print("device =", args.device)

    if args.yesterday:
        if args.csv is None:
            sys.exit("--yesterday ëª¨ë“œì—ëŠ” --csv ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        df_tmp = pd.read_csv(args.csv)
        member_id = (df_tmp.get("memberID",
                                pd.Series(["unknown"]))
        .iloc[0])
        reconcile_day_duplicates(args.csv, member_id)
        return

    if args.tomorrow:
        if args.csv is None or args.model is None:
            sys.exit("âŒ --tomorrow ëª¨ë“œì—ì„œëŠ” --csv ì™€ --model ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        if not args.csv.exists():
            sys.exit(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ â†’ {args.csv}")
        if not args.model.exists():
            sys.exit(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ â†’ {args.model}")
        # ë‚´ì¼ ì˜ˆì¸¡
        series = predict_tomorrow(
            member_id=args.member_id or "unknown",
            hist_csv=args.csv, model_path=args.model,
            device=args.device)
        print(series)
        return

    if args.pretrain_folder is not None:
        args.win = win
        args.label = label
        args.pred = pred
        pretrain_on_folder(
            args.pretrain_folder,
            args
        )
        return
    if args.model and args.csv is not None:
        # ì˜ˆì¸¡ ì „ìš© ëª¨ë“œ: ì €ì¥ëœ ëª¨ë¸ë¡œ CSV ì˜ˆì¸¡
        predict_from_csv(
            args.csv, args.model, n_blocks=args.n_pred_blocks,
            device=args.device)
        return

    if args.finetune and args.csv is not None:
        # íŒŒì¸íŠœë‹ ì „ìš© ëª¨ë“œ: ì €ì¥ëœ ëª¨ë¸ë¡œ CSV íŒŒì¸íŠœë‹
        if not args.finetune.exists():
            sys.exit(f"ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤ â†’ {args.finetune}")

        state = torch.load(args.finetune, map_location=args.device)
        pretrained_mean = state["scaler_mean"]
        pretrained_scale = state["scaler_scale"]

        model = TransformerRevIN(
            d_model=160, input_features_count=7,
            num_encoder_layers=3, num_decoder_layers=3,
            dim_feedforward=160, kernel_size=3,
            dropout=0.1, attention_heads=8
        ).to(args.device)
        model.load_state_dict(state["weights"])
        model.relu = nn.Identity()  # ReLU ì œê±°

    df = pd.read_csv(args.csv)
    df["mrdDt"] = pd.to_datetime(df["mrdDt"], format="%Y-%m-%d %H")
    df = df.sort_values("mrdDt").reset_index(drop=True)
    df["date"] = df["mrdDt"].dt.date
    member_id = df.get("memberID", pd.Series(["unknown"])).iloc[0]

    # === ADD BEGIN : predict_only ë¹ ë¥¸ íƒˆì¶œ ======================
    if args.predict_only:
        ckpt = args.save_dir / f"{args.member_id or member_id}.pt"
        if not ckpt.exists():
            sys.exit(f"ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤ â†’ {ckpt}")
        state = torch.load(ckpt, map_location=args.device)
        model = TransformerRevIN(d_model=160, input_features_count=7,
                                 num_encoder_layers=3, num_decoder_layers=3,
                                 dim_feedforward=160, kernel_size=3,
                                 dropout=0.1, attention_heads=8).to(args.device)
        model.load_state_dict(state["weights"])
        scaler = StandardScaler()
        scaler.mean_, scaler.scale_ = state["scaler_mean"], state["scaler_scale"]
        predict_blocks(model, df, scaler,
                       win, label, pred, args.n_pred_blocks, args.device)
        return
    # === ADD END =================================================

    # (ì•„ë˜ í•™ìŠµ ë£¨í”„ëŠ” ë‹¹ì‹  ì½”ë“œ ê·¸ëŒ€ë¡œ â€¦)

    dates = df["date"].unique()
    blocks = [dates[i:i + block_days]
              for i in range(0, len(dates), block_days)
              if len(dates[i:i + block_days]) == block_days]

    tscv = TimeSeriesSplit(n_splits=args.kfold, test_size=1)
    fold_results = []
    for fold, (tr_idx, val_idx) in enumerate(tscv.split(blocks), 1):
        train_dates = np.concatenate([blocks[i] for i in tr_idx])
        val_dates = np.concatenate([blocks[i] for i in val_idx])
        next_block = val_idx[-1] + 1
        if next_block < len(blocks):
            test_dates = blocks[next_block]
        else:
            test_dates = blocks[val_idx[-1]]

        tr_df = df[df["date"].isin(train_dates)].drop(columns="date")
        va_df = df[df["date"].isin(val_dates)].drop(columns="date")
        te_df = df[df["date"].isin(test_dates)].drop(columns="date")

        print(f"\n========== Fold {fold}/{args.kfold} ==========")
        print(f"Train {train_dates[0]} ~ {train_dates[-1]}")
        print(f" Val  {val_dates[0]} ~ {val_dates[-1]}")
        print(f" Test {test_dates[0]} ~ {test_dates[-1]}")

        scaler = StandardScaler()
        tr_dl = DataLoader(PowerDataset(tr_df, scaler, win, label, pred, fit=True),
                           batch_size=args.batch, shuffle=True)
        va_dl = DataLoader(PowerDataset(va_df, scaler, win, label, pred),
                           batch_size=args.batch)
        te_dl = DataLoader(PowerDataset(te_df, scaler, win, label, pred),
                           batch_size=args.batch)

        model = TransformerRevIN(d_model=160, input_features_count=7,
                                 num_encoder_layers=3, num_decoder_layers=3,
                                 dim_feedforward=160, kernel_size=3,
                                 dropout=0.05, attention_heads=8).to(args.device)
        model.relu = nn.Identity()

        crit = L1Loss();
        opt = AdamW(model.parameters(), lr=5e-4, eps=1e-9)
        sch = StepLR(opt, step_size=2, gamma=0.1)
        mask = causal_mask(label + pred, args.device)

        best, pat = 1e9, 0
        for ep in range(1, args.epochs + 1):
            tr = run_epoch(model, tr_dl, crit, opt, mask, args.device)
            va = run_epoch(model, va_dl, crit, None, mask, args.device)
            sch.step()
            va_real = eval_epoch_real(model, va_dl, scaler, mask, args.device)
            print(f"[{ep:03d}] train(norm)={tr:.4f}  val(norm)={va:.4f}  val(kW)={va_real:.4f}")
            print(preview_batch(model, va_dl, mask, scaler, args.device), "\n")
            if va < best - 1e-4:
                best, best_state, pat = va, model.state_dict(), 0
            else:
                pat += 1
                if pat > 8: print("Early stop."); break
        model.load_state_dict(best_state)
        test_mae = run_epoch(model, te_dl, crit, None, mask, args.device)
        fold_results.append({"fold": fold, "val": best, "test": test_mae})
        print(f"Fold {fold}  best_val={best:.4f} | test_MAE={test_mae:.4f}")

    print("\n===== K-Fold Summary =====")
    for r in fold_results:
        print(f"Fold {r['fold']}: val={r['val']:.4f} | test={r['test']:.4f}")
    print(f"Avg  val={np.mean([r['val'] for r in fold_results]):.4f}")
    print(f"Avg test={np.mean([r['test'] for r in fold_results]):.4f}")

    # === ADD BEGIN : ëª¨ë¸ ì €ì¥ ====================================
    args.save_dir.mkdir(parents=True, exist_ok=True)
    ckpt = args.save_dir / f"{member_id}.pt"
    torch.save({"weights": model.state_dict(),
                "scaler_mean": scaler.mean_,
                "scaler_scale": scaler.scale_}, ckpt)
    print(f"\nëª¨ë¸ ì €ì¥ ì™„ë£Œ â†’ {ckpt}")
    # === ADD END ==================================================

    model.load_state_dict(best_state)
    model.eval()

    residuals = []
    with torch.no_grad():
        for enc, dec, tgt in va_dl:
            enc, dec, tgt = enc.to(args.device), dec.to(args.device), tgt.to(args.device)
            out = model(enc, dec, tgt_mask=mask)
            # ë””ì½”ë” ê¸¸ì´ ë§ì¶”ê¸°
            if out.size(1) != tgt.size(1):
                out = out[:, -tgt.size(1):]
            # ì› ìŠ¤ì¼€ì¼ë¡œ ë³µì›
            out = out.cpu().numpy() * scaler.scale_[0] + scaler.mean_[0]
            tgt = tgt.cpu().numpy() * scaler.scale_[0] + scaler.mean_[0]
            residuals.append(np.abs(out - tgt))

    residuals = np.concatenate(residuals).ravel()
    thr_97 = np.percentile(residuals, 97)
    print(f"\n99% ì”ì°¨ ì„ê³„ê°’: {thr_97:.4f} kW")
    '''
    eval_random_10_blocks(model, df, scaler,
                      win, label, pred,
                      device=args.device)
                      '''
    print(f"\n97% ì”ì°¨ ì„ê³„ê°’: {thr_97:.4f} kW")
    # ì¶”ê°€: ì¦‰ì‹œ ì˜ˆì¸¡ ìƒ˜í”Œ í™•ì¸
    predict_blocks(model, df, scaler,
                   win, label, pred,
                   n_blocks=15, device=args.device)



if __name__ == "__main__":
    main()